{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "import os \n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "import io_meld as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "# Define the subject directory and feature names\n",
    "subject_dir = 'pathto'\n",
    "feature_names = ['gm_FLAIR_0.75', 'gm_FLAIR_0.5', 'gm_FLAIR_0.25', 'gm_FLAIR_0', 'wm_FLAIR_0.5', 'wm_FLAIR_1',]\n",
    "# Path to the file containing the list of subjects\n",
    "subject_list_path = 'pathto/subject_list2.txt'\n",
    "# Read the list of subjects from the file\n",
    "with open(subject_list_path, 'r') as file:\n",
    "    subjects = [line.strip() for line in file if line.strip()]\n",
    "# Path to the cortex file\n",
    "cortex_file = 'pathto/fsaverage_sym/label/lh.cortex.label'\n",
    "# Load cortex mask\n",
    "cortex = nib.freesurfer.read_label(cortex_file)  # Using nibabel to read Freesurfer label files directly\n",
    "# Function to load feature data\n",
    "def load_feature_data(subject, feature):\n",
    "    lh_file = os.path.join(subject_dir, subject, 'xhemi', 'surf_meld', f'lh.on_lh.{feature}.mgh')\n",
    "    rh_file = os.path.join(subject_dir, subject, 'xhemi', 'surf_meld', f'rh.on_lh.{feature}.mgh')\n",
    "    lh_data = nib.load(lh_file).get_fdata()\n",
    "    rh_data = nib.load(rh_file).get_fdata()\n",
    "    return lh_data, rh_data\n",
    "# Process each subject\n",
    "for subject in subjects:\n",
    "    for feature in feature_names:\n",
    "        # Load feature data\n",
    "        lh_feature, rh_feature = load_feature_data(subject, feature)\n",
    "        # Create full-size arrays filled with NaNs\n",
    "        lh_full = np.full(lh_feature.shape, np.nan)\n",
    "        rh_full = np.full(rh_feature.shape, np.nan)\n",
    "        # Assign cortex values to the full-size arrays\n",
    "        lh_full[cortex] = lh_feature[cortex]\n",
    "        rh_full[cortex] = rh_feature[cortex]\n",
    "        # Compute mean and standard deviation ignoring NaNs\n",
    "        concatenated_features = np.concatenate([lh_full[cortex], rh_full[cortex]])\n",
    "        mu_sub = np.nanmean(concatenated_features)\n",
    "        std_sub = np.nanstd(concatenated_features)\n",
    "        # Normalize the features\n",
    "        z_lh_feature = (lh_full - mu_sub) / std_sub\n",
    "        z_rh_feature = (rh_full - mu_sub) / std_sub\n",
    "        # Load one of the original MGH files to use as a template for saving\n",
    "        template_mgh_file = os.path.join(subject_dir, subject, 'xhemi', 'surf_meld', f'lh.on_lh.{feature}.mgh')\n",
    "        if not os.path.exists(template_mgh_file):\n",
    "            template_mgh_file = os.path.join(subject_dir, subject, 'xhemi', 'surf_meld', f'rh.on_lh.{feature}.mgh')\n",
    "        template_mgh = nib.load(template_mgh_file)\n",
    "        template_affine = template_mgh.affine\n",
    "        template_header = template_mgh.header\n",
    "        # Create new MGHImage objects for Z-scores\n",
    "        z_lh_img = nib.MGHImage(z_lh_feature, affine=template_affine, header=template_header)\n",
    "        z_rh_img = nib.MGHImage(z_rh_feature, affine=template_affine, header=template_header)\n",
    "        # Define output paths\n",
    "        lh_outfile = os.path.join(subject_dir, subject, f'lh.intra_z.{feature}.mgh')\n",
    "        rh_outfile = os.path.join(subject_dir, subject, f'rh.intra_z.{feature}.mgh')\n",
    "        # Save the Z-score data to MGH files\n",
    "        nib.save(z_lh_img, lh_outfile)\n",
    "        nib.save(z_rh_img, rh_outfile)\n",
    "        print(f\"Z-score data for {subject} - {feature} has been saved to '{lh_outfile}' and '{rh_outfile}'.\")\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Directory to save control subject data\n",
    "output_dir = 'path/MELD_control'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read the list of subjects from the file\n",
    "with open(subject_list_path, 'r') as file:\n",
    "    subjects = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# Function to load MGH files for a specific feature\n",
    "def load_feature_data(subject_dir, subjects, feature_name):\n",
    "    contralateral_data_list = []\n",
    "    \n",
    "    for subject in subjects:\n",
    "        lh_lesion_file = os.path.join(subject_dir, subject, 'lh.lesion.nii')\n",
    "        rh_lesion_file = os.path.join(subject_dir, subject, 'rh.lesion.nii')\n",
    "        \n",
    "        if os.path.exists(lh_lesion_file):\n",
    "            contralateral_feature_file = os.path.join(subject_dir, subject, f'rh.intra_z.{feature_name}.mgh')\n",
    "        elif os.path.exists(rh_lesion_file):\n",
    "            contralateral_feature_file = os.path.join(subject_dir, subject, f'lh.intra_z.{feature_name}.mgh')\n",
    "        else:\n",
    "            print(f\"Warning: No lesion mask found for subject {subject}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        if os.path.exists(contralateral_feature_file):\n",
    "            contralateral_mgh_data = nib.load(contralateral_feature_file).get_fdata()\n",
    "            contralateral_data_list.append(contralateral_mgh_data)\n",
    "        else:\n",
    "            print(f\"Warning: {contralateral_feature_file} does not exist and will be skipped.\")\n",
    "    \n",
    "    if contralateral_data_list:\n",
    "        return np.stack(contralateral_data_list, axis=-1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Process each feature for intersubject normalization\n",
    "for feature in feature_names:\n",
    "    # Load the data for the current feature\n",
    "    data_array = load_feature_data(subject_dir, subjects, feature)\n",
    "    \n",
    "    if data_array is not None:\n",
    "        # Calculate mean and standard deviation along the subjects axis, ignoring NaNs\n",
    "        mean_intensity = np.nanmean(data_array, axis=-1)\n",
    "        sd_intensity = np.nanstd(data_array, axis=-1)\n",
    "        \n",
    "        # Load one of the original MGH files to use as a template for saving\n",
    "        template_mgh_file = os.path.join(subject_dir, subjects[0], f'lh.intra_z.{feature}.mgh')\n",
    "        if not os.path.exists(template_mgh_file):\n",
    "            template_mgh_file = os.path.join(subject_dir, subjects[0], f'rh.intra_z.{feature}.mgh')\n",
    "        \n",
    "        template_mgh = nib.load(template_mgh_file)\n",
    "        template_affine = template_mgh.affine\n",
    "        template_header = template_mgh.header\n",
    "\n",
    "        # Create new MGHImage objects for mean and SD\n",
    "        mean_img = nib.MGHImage(mean_intensity, affine=template_affine, header=template_header)\n",
    "        sd_img = nib.MGHImage(sd_intensity, affine=template_affine, header=template_header)\n",
    "\n",
    "        # Define output paths\n",
    "        mean_output_path = os.path.join(output_dir, f'mu.intra_z.{feature}.mgh')\n",
    "        sd_output_path = os.path.join(output_dir, f'std.intra_z.{feature}.mgh')\n",
    "\n",
    "        # Save the mean and SD data to MGH files\n",
    "        nib.save(mean_img, mean_output_path)\n",
    "        nib.save(sd_img, sd_output_path)\n",
    "\n",
    "        print(f\"Control subject data for {feature} has been saved to '{mean_output_path}' and '{sd_output_path}'.\")\n",
    "    else:\n",
    "        print(f\"No data found for feature {feature} across all subjects.\")\n",
    "\n",
    "# Adjust each patient's intra_z feature using the template mu and std\n",
    "for subject in subjects:\n",
    "    for feature in feature_names:\n",
    "        # Paths to the mu and std files\n",
    "        mu_file = os.path.join(output_dir, f'mu.intra_z.{feature}.mgh')\n",
    "        std_file = os.path.join(output_dir, f'std.intra_z.{feature}.mgh')\n",
    "        \n",
    "        # Load the template mu and std\n",
    "        mu_intra_cohort = nib.load(mu_file).get_fdata().squeeze()\n",
    "        std_intra_cohort = nib.load(std_file).get_fdata().squeeze()\n",
    "        \n",
    "        # Paths to the subject's intra_z files\n",
    "        lh_intra_file = os.path.join(subject_dir, subject, f'lh.intra_z.{feature}.mgh')\n",
    "        rh_intra_file = os.path.join(subject_dir, subject, f'rh.intra_z.{feature}.mgh')\n",
    "        \n",
    "        if os.path.exists(lh_intra_file) and os.path.exists(rh_intra_file):\n",
    "            # Load the subject's intra_z files\n",
    "            lh_intra_sub = nib.load(lh_intra_file).get_fdata().squeeze()\n",
    "            rh_intra_sub = nib.load(rh_intra_file).get_fdata().squeeze()\n",
    "            \n",
    "            # Z-scoring with NaNs ignored\n",
    "            lh_inter_intra_sub = (lh_intra_sub - mu_intra_cohort) / std_intra_cohort\n",
    "            rh_inter_intra_sub = (rh_intra_sub - mu_intra_cohort) / std_intra_cohort\n",
    "            \n",
    "            # Create new MGHImage objects for intersubject normalized data\n",
    "            lh_inter_img = nib.MGHImage(lh_inter_intra_sub, affine=template_affine, header=template_header)\n",
    "            rh_inter_img = nib.MGHImage(rh_inter_intra_sub, affine=template_affine, header=template_header)\n",
    "            \n",
    "            # Define output paths\n",
    "            lh_inter_output_path = os.path.join(subject_dir, subject, f'lh.inter_z.intra_z.{feature}.mgh')\n",
    "            rh_inter_output_path = os.path.join(subject_dir, subject, f'rh.inter_z.intra_z.{feature}.mgh')\n",
    "            \n",
    "            # Save the intersubject normalized data to MGH files\n",
    "            nib.save(lh_inter_img, lh_inter_output_path)\n",
    "            nib.save(rh_inter_img, rh_inter_output_path)\n",
    "            \n",
    "            print(f\"Intersubject normalized data for {subject} - {feature} has been saved to '{lh_inter_output_path}' and '{rh_inter_output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_lesion_mask(subject, subject_dir):\n",
    "    \"\"\"\n",
    "    Loads the lesion mask for the given subject from either the left or right hemisphere.\n",
    "    Returns the lesion mask, lesion hemisphere, and contralateral hemisphere.\n",
    "    \"\"\"\n",
    "    lesion_subject = subject.replace('_C_', '_T_') if '_C_' in subject else subject\n",
    "\n",
    "    # Define paths for the lesion mask in both hemispheres\n",
    "    lesion_mask_path_lh = os.path.join(subject_dir, lesion_subject, 'xhemi', 'surf_meld', 'lh.on_lh.lesion.mgh')\n",
    "    lesion_mask_path_rh = os.path.join(subject_dir, lesion_subject, 'xhemi', 'surf_meld', 'rh.on_lh.lesion.mgh')\n",
    "\n",
    "    if os.path.exists(lesion_mask_path_lh):\n",
    "        lesion_hemi = 'lh'\n",
    "        contralateral_hemi = 'rh'\n",
    "        lesion_mask_path = lesion_mask_path_lh\n",
    "    elif os.path.exists(lesion_mask_path_rh):\n",
    "        lesion_hemi = 'rh'\n",
    "        contralateral_hemi = 'lh'\n",
    "        lesion_mask_path = lesion_mask_path_rh\n",
    "    else:\n",
    "        print(f\"No valid lesion mask file found for {subject}\")\n",
    "        return None, None, None\n",
    "\n",
    "    try:\n",
    "        lesion_mask = nib.load(lesion_mask_path).get_fdata() > 0\n",
    "        \n",
    "        return lesion_mask, lesion_hemi, contralateral_hemi\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading lesion mask for {subject}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def lesion_zscore(subject, subject_dir, feature_name):\n",
    "    \"\"\"\n",
    "    Calculates the lesion z-scores for a specific feature of a subject.\n",
    "    \"\"\"\n",
    "    z_scores = {'lesion': {}, 'lesion_contra': {}}\n",
    "\n",
    "    # Load the lesion mask and determine hemispheres\n",
    "    lesion_mask, lesion_hemi, contralateral_hemi = load_lesion_mask(subject, subject_dir)\n",
    "    if lesion_mask is None:\n",
    "        return None\n",
    "\n",
    "    # Construct paths for z-scores based on feature names\n",
    "    z_score_path_lesion = os.path.join(subject_dir, subject, f'{lesion_hemi}.inter_z.intra_z.{feature_name}.mgh')\n",
    "    z_score_path_contra = os.path.join(subject_dir, subject, f'{contralateral_hemi}.inter_z.intra_z.{feature_name}.mgh')\n",
    "\n",
    "    # Load z-scores for the lesion hemisphere and apply the lesion mask\n",
    "    if os.path.exists(z_score_path_lesion):\n",
    "        try:\n",
    "            z_scores_lesion = nib.load(z_score_path_lesion).get_fdata()\n",
    "            if z_scores_lesion.shape == lesion_mask.shape:\n",
    "                lesion_z_score_values = z_scores_lesion[lesion_mask]\n",
    "                z_scores['lesion'][lesion_hemi] = np.nanmean(lesion_z_score_values)\n",
    "            else:\n",
    "                print(f\"Shape mismatch for {subject} in {lesion_hemi}: lesion mask {lesion_mask.shape}, z-scores {z_scores_lesion.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading lesion z-score for {subject}: {e}\")\n",
    "    else:\n",
    "        print(f\"Lesion z-score file not found for {subject} in {lesion_hemi} with feature {feature_name} at path {z_score_path_lesion}\")\n",
    "\n",
    "    # Load z-scores for the contralateral hemisphere and apply the lesion mask\n",
    "    if os.path.exists(z_score_path_contra):\n",
    "        try:\n",
    "            z_scores_contra = nib.load(z_score_path_contra).get_fdata()\n",
    "            if z_scores_contra.shape == lesion_mask.shape:\n",
    "                lesion_contra_z_score_values = z_scores_contra[lesion_mask]\n",
    "                z_scores['lesion_contra'][contralateral_hemi] = np.nanmean(lesion_contra_z_score_values)\n",
    "            else:\n",
    "                print(f\"Shape mismatch for {subject} in {contralateral_hemi}: lesion mask {lesion_mask.shape}, z-scores {z_scores_contra.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading contralateral z-score for {subject}: {e}\")\n",
    "    else:\n",
    "        print(f\"Contralateral z-score file not found for {subject} in {contralateral_hemi} with feature {feature_name} at path {z_score_path_contra}\")\n",
    "\n",
    "    # Check if any scores were calculated\n",
    "    return z_scores if z_scores['lesion'] or z_scores['lesion_contra'] else None\n",
    "\n",
    "def process_subjects(subjects, subject_dir, feature_names):\n",
    "    results = []\n",
    "    for subject in subjects:\n",
    "        for feature in feature_names:\n",
    "            z_scores = lesion_zscore(subject, subject_dir, feature)\n",
    "            if z_scores:\n",
    "                for hemisphere, z_score in z_scores['lesion'].items():\n",
    "                    results.append([subject, feature, hemisphere, 'lesion', z_score])\n",
    "                for hemisphere, z_score in z_scores['lesion_contra'].items():\n",
    "                    results.append([subject, feature, hemisphere, 'lesion_contra', z_score])\n",
    "            else:\n",
    "                print(f\"No valid z-scores calculated for {subject} with feature {feature}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define subject directory\n",
    "subject_dir = '/Users/pierceburr/Documents/Documents/UCL_dissertation/meld/output/fastsurfer'\n",
    "\n",
    "# Process subjects and save results to CSV\n",
    "results = process_subjects(subjects, subject_dir, feature_names)\n",
    "\n",
    "# Define the output directory and CSV file path\n",
    "output_dir = os.path.join(subject_dir, 'inter.intra_z')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "csv_file_path = os.path.join(output_dir, 'results_inter_intra_z_with_contralateral_lesion_mask_FLAIR2.csv')\n",
    "\n",
    "# Write the results to a CSV file\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Subject', 'Feature', 'Hemisphere', 'Side', 'Z-Score'])\n",
    "    writer.writerows(results)\n",
    "\n",
    "# Output CSV path for easy access\n",
    "print(f\"Results saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the Z-score data\n",
    "z_scores_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Load the age data\n",
    "age_df = pd.read_csv('pathto/MELD_age.csv')\n",
    "\n",
    "# Strip leading/trailing spaces from column names\n",
    "z_scores_df.columns = z_scores_df.columns.str.strip()\n",
    "age_df.columns = age_df.columns.str.strip()\n",
    "\n",
    "# Strip spaces from text data in 'Subject' column if necessary\n",
    "z_scores_df['Subject'] = z_scores_df['Subject'].str.strip()\n",
    "age_df['Subject'] = age_df['Subject'].str.strip()\n",
    "\n",
    "# Ensure 'Z-Score' column is numeric, handle potential non-string types\n",
    "z_scores_df['Z-Score'] = z_scores_df['Z-Score'].apply(lambda x: np.fromstring(str(x).strip('[]'), sep=',') if pd.notnull(x) else np.nan)\n",
    "\n",
    "# Take the mean of the Z-Score lists\n",
    "z_scores_df['Z-Score'] = z_scores_df['Z-Score'].apply(lambda x: np.nanmean(x) if isinstance(x, np.ndarray) else np.nan)\n",
    "\n",
    "# Merge the age DataFrame with the Z-score DataFrame on 'Subject'\n",
    "merged_df = pd.merge(z_scores_df, age_df, on='Subject', how='left')\n",
    "\n",
    "# Create a simplified subject identifier by removing '_C' and '_T'\n",
    "merged_df['Simplified_Subject'] = merged_df['Subject'].str.replace(r'_C_|_T_', '_', regex=True)\n",
    "\n",
    "# Separate columns for Age and Z-Score depending on '_C_' or '_T_'\n",
    "merged_df['Age1'] = merged_df.apply(lambda x: x['Age'] if '_C_' in x['Subject'] else None, axis=1)\n",
    "merged_df['Age2'] = merged_df.apply(lambda x: x['Age'] if '_T_' in x['Subject'] else None, axis=1)\n",
    "merged_df['Lesion1'] = merged_df.apply(lambda x: x['Z-Score'] if '_C_' in x['Subject'] and x['Side'] == 'lesion' else None, axis=1)\n",
    "merged_df['Lesion2'] = merged_df.apply(lambda x: x['Z-Score'] if '_T_' in x['Subject'] and x['Side'] == 'lesion' else None, axis=1)\n",
    "merged_df['LesionContra1'] = merged_df.apply(lambda x: x['Z-Score'] if '_C_' in x['Subject'] and x['Side'] == 'lesion_contra' else None, axis=1)\n",
    "merged_df['LesionContra2'] = merged_df.apply(lambda x: x['Z-Score'] if '_T_' in x['Subject'] and x['Side'] == 'lesion_contra' else None, axis=1)\n",
    "\n",
    "# Group by the new subject identifier and the actual 'Feature' column, then aggregate the data\n",
    "agg_df = merged_df.groupby(['Simplified_Subject', 'Feature']).agg({\n",
    "    'Age1': 'max',  # Using max to handle None values and get a single value per group\n",
    "    'Age2': 'max',\n",
    "    'Lesion1': 'max',\n",
    "    'Lesion2': 'max',\n",
    "    'LesionContra1': 'max',\n",
    "    'LesionContra2': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "# Save the aggregated DataFrame to a new CSV file\n",
    "output_file_path = 'pathto/FLAIR_age_z_with_contralateral.csv'\n",
    "agg_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Aggregated data with lesion and contralateral lesion Z-scores saved to {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
